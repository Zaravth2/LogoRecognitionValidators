{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook by Zara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``CNN using merged dataset``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/zaravanthoff/Desktop/MasterProject/Datasets/full_dataset/full_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LITTLE CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>class</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>class_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3151369135636477165_jpg.rf.521cb85f2777fe14c53...</td>\n",
       "      <td>480.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>NIKE</td>\n",
       "      <td>352</td>\n",
       "      <td>507</td>\n",
       "      <td>370</td>\n",
       "      <td>538</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>youtube-44_jpg.rf.6e4b9a30c5c74280ee120b18193d...</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>NIKE</td>\n",
       "      <td>877</td>\n",
       "      <td>441</td>\n",
       "      <td>963</td>\n",
       "      <td>469</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>youtube-44_jpg.rf.6e4b9a30c5c74280ee120b18193d...</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>NIKE</td>\n",
       "      <td>316</td>\n",
       "      <td>661</td>\n",
       "      <td>337</td>\n",
       "      <td>680</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2746419871990892444_jpg.rf.76034c0a0150b72a5a2...</td>\n",
       "      <td>480.0</td>\n",
       "      <td>853.0</td>\n",
       "      <td>NIKE</td>\n",
       "      <td>223</td>\n",
       "      <td>510</td>\n",
       "      <td>232</td>\n",
       "      <td>515</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2746419871990892444_jpg.rf.76034c0a0150b72a5a2...</td>\n",
       "      <td>480.0</td>\n",
       "      <td>853.0</td>\n",
       "      <td>NIKE</td>\n",
       "      <td>160</td>\n",
       "      <td>649</td>\n",
       "      <td>166</td>\n",
       "      <td>655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18254</th>\n",
       "      <td>2126991906.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>253</td>\n",
       "      <td>54</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18255</th>\n",
       "      <td>217288720.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>136</td>\n",
       "      <td>161</td>\n",
       "      <td>304</td>\n",
       "      <td>222</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18256</th>\n",
       "      <td>2472817996.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>499</td>\n",
       "      <td>106</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18257</th>\n",
       "      <td>2514220918.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>342</td>\n",
       "      <td>157</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18258</th>\n",
       "      <td>386891249.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yahoo</td>\n",
       "      <td>156</td>\n",
       "      <td>10</td>\n",
       "      <td>310</td>\n",
       "      <td>49</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18259 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                filename   width  height  \\\n",
       "0      3151369135636477165_jpg.rf.521cb85f2777fe14c53...   480.0   600.0   \n",
       "1      youtube-44_jpg.rf.6e4b9a30c5c74280ee120b18193d...  1920.0  1080.0   \n",
       "2      youtube-44_jpg.rf.6e4b9a30c5c74280ee120b18193d...  1920.0  1080.0   \n",
       "3      2746419871990892444_jpg.rf.76034c0a0150b72a5a2...   480.0   853.0   \n",
       "4      2746419871990892444_jpg.rf.76034c0a0150b72a5a2...   480.0   853.0   \n",
       "...                                                  ...     ...     ...   \n",
       "18254                                     2126991906.jpg     NaN     NaN   \n",
       "18255                                      217288720.jpg     NaN     NaN   \n",
       "18256                                     2472817996.jpg     NaN     NaN   \n",
       "18257                                     2514220918.jpg     NaN     NaN   \n",
       "18258                                      386891249.jpg     NaN     NaN   \n",
       "\n",
       "       class  xmin  ymin  xmax  ymax  class_index  \n",
       "0       NIKE   352   507   370   538            1  \n",
       "1       NIKE   877   441   963   469            1  \n",
       "2       NIKE   316   661   337   680            1  \n",
       "3       NIKE   223   510   232   515            1  \n",
       "4       NIKE   160   649   166   655            1  \n",
       "...      ...   ...   ...   ...   ...          ...  \n",
       "18254  Yahoo    15     6   253    54           76  \n",
       "18255  Yahoo   136   161   304   222           76  \n",
       "18256  Yahoo     2     4   499   106           76  \n",
       "18257  Yahoo     1    69   342   157           76  \n",
       "18258  Yahoo   156    10   310    49           76  \n",
       "\n",
       "[18259 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.drop(columns=['width', 'height'], inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path containing the images\n",
    "folder_path = \"/Users/zaravanthoff/Desktop/MasterProject/Datasets/full_dataset/full_images(2)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each row in the dataset\n",
    "for index, row in data.iterrows():\n",
    "    # Extract image filename from the dataset\n",
    "    image_filename = row.iloc[0]  # Accessing by position using iloc\n",
    "                            # THis ensures that values are accessed by position rather than by integer index labels.\n",
    "    \n",
    "    # Construct the full path to the image\n",
    "    image_path = os.path.join(folder_path, image_filename)\n",
    "    \n",
    "    # Read the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # # Display the image (you can perform any processing here)\n",
    "    # cv2.imshow(\"Image\", image)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of logo classes: 76\n"
     ]
    }
   ],
   "source": [
    "num_classes = data['class'].nunique()\n",
    "print(\"Number of logo classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target size for resizing\n",
    "target_height = 100\n",
    "target_width = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store resized images and corresponding labels\n",
    "resized_images = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bs/xnv9qfhn03xdhr_rlq0jjbq40000gn/T/ipykernel_17963/2636375599.py:19: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  label = row[1]\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each row in the dataset\n",
    "for index, row in data.iterrows():\n",
    "    # Extract image filename from the dataset\n",
    "    image_filename = row.iloc[0]  # Accessing by integer index 0\n",
    "    \n",
    "    # Construct the full path to the image\n",
    "    image_path = os.path.join(folder_path, image_filename)\n",
    "    \n",
    "    # Read the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Resize the image to the target size\n",
    "    resized_image = cv2.resize(image, (target_width, target_height))\n",
    "    \n",
    "    # Append resized image to the list\n",
    "    resized_images.append(resized_image)\n",
    "    \n",
    "    # Append label to the list (assuming label is in the second column of the dataframe)\n",
    "    label = row[1]\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to numpy arrays\n",
    "resized_images = np.array(resized_images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized Images Shape: (36518, 100, 100, 3)\n",
      "Labels Shape: (36518,)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of resized images and labels\n",
    "print(\"Resized Images Shape:\", resized_images.shape)\n",
    "print(\"Labels Shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NORMALIZATION OF PIXEL VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Pixel Value After Normalization: 0.0\n",
      "Maximum Pixel Value After Normalization: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Normalize pixel values\n",
    "resized_images = resized_images.astype('float32') / 255.0\n",
    "\n",
    "# Check the range of pixel values after normalization\n",
    "print(\"Minimum Pixel Value After Normalization:\", np.min(resized_images))\n",
    "print(\"Maximum Pixel Value After Normalization:\", np.max(resized_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample a subset of the dataset\n",
    "num_samples_to_keep = 10000  # Adjust this number based on your requirements\n",
    "num_instances = len(resized_images)\n",
    "sampled_indices = np.random.choice(num_instances, num_samples_to_keep, replace=False)\n",
    "sampled_images = resized_images[sampled_indices]\n",
    "sampled_labels = labels[sampled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the sampled subset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sampled_images, sampled_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape (X_train): (8000, 100, 100, 3)\n",
      "Training set shape (y_train): (8000,)\n",
      "Testing set shape (X_test): (2000, 100, 100, 3)\n",
      "Testing set shape (y_test): (2000,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the training and testing sets\n",
    "print(\"Training set shape (X_train):\", X_train.shape)\n",
    "print(\"Training set shape (y_train):\", y_train.shape)\n",
    "print(\"Testing set shape (X_test):\", X_test.shape)\n",
    "print(\"Testing set shape (y_test):\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of logo classes in your dataset\n",
    "num_classes = len(data['class'].unique())\n",
    "\n",
    "# Ensure num_classes is an integer\n",
    "if not isinstance(num_classes, int):\n",
    "    num_classes = int(num_classes)\n",
    "\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zaravanthoff/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n",
      "2024-03-21 16:41:18.629505: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-03-21 16:41:18.629638: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-03-21 16:41:18.629649: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-03-21 16:41:18.630262: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-03-21 16:41:18.630840: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    # Convolutional layers\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_height, target_width, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Flatten layer to convert 3D feature maps to 1D feature vectors\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    # Fully connected layers\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),  # Dropout layer to reduce overfitting\n",
    "    layers.Dense(num_classes, activation='softmax')  # Output layer with softmax activation for multi-class classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform labels in training set\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Transform labels in testing set (using the same encoder)\n",
    "# Ensure consistency of labels in testing set\n",
    "y_test_unique = set(y_test)\n",
    "unseen_labels = y_test_unique - set(label_encoder.classes_)\n",
    "if unseen_labels:\n",
    "    raise ValueError(f\"Unseen labels in testing set: {unseen_labels}\")\n",
    "y_test_encoded = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform labels in training set\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Transform labels in testing set (using the same encoder)\n",
    "y_test_encoded = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 16:42:13.729102: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 61ms/step - accuracy: 0.7051 - loss: 1.8001 - val_accuracy: 0.7830 - val_loss: 0.9313\n",
      "Epoch 2/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 56ms/step - accuracy: 0.8091 - loss: 0.8387 - val_accuracy: 0.8550 - val_loss: 0.5737\n",
      "Epoch 3/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - accuracy: 0.8870 - loss: 0.4670 - val_accuracy: 0.9135 - val_loss: 0.3912\n",
      "Epoch 4/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 54ms/step - accuracy: 0.9388 - loss: 0.2603 - val_accuracy: 0.9315 - val_loss: 0.3448\n",
      "Epoch 5/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - accuracy: 0.9487 - loss: 0.1833 - val_accuracy: 0.9345 - val_loss: 0.3645\n",
      "Epoch 6/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 54ms/step - accuracy: 0.9605 - loss: 0.1438 - val_accuracy: 0.9455 - val_loss: 0.2936\n",
      "Epoch 7/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 55ms/step - accuracy: 0.9654 - loss: 0.1230 - val_accuracy: 0.9470 - val_loss: 0.3151\n",
      "Epoch 8/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 58ms/step - accuracy: 0.9668 - loss: 0.1190 - val_accuracy: 0.9455 - val_loss: 0.2996\n",
      "Epoch 9/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 56ms/step - accuracy: 0.9728 - loss: 0.1010 - val_accuracy: 0.9490 - val_loss: 0.3361\n",
      "Epoch 10/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 56ms/step - accuracy: 0.9741 - loss: 0.0914 - val_accuracy: 0.9505 - val_loss: 0.3486\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train_encoded, epochs=10, validation_data=(X_test, y_test_encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9473 - loss: 0.3891\n",
      "Test accuracy: 0.9505000114440918\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_encoded)\n",
    "\n",
    "# Print the test accuracy\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Predicted logo: Google\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def predict_logo(image_path, model, label_encoder):\n",
    "    # Load and preprocess the image\n",
    "    img = image.load_img(image_path, target_size=(target_height, target_width))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = img_array.reshape((1, target_height, target_width, 3))\n",
    "    img_array = img_array / 255.0  # Normalize pixel values\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(img_array)\n",
    "    \n",
    "    # Decode the prediction\n",
    "    predicted_label = label_encoder.inverse_transform([np.argmax(prediction)])\n",
    "    \n",
    "    return predicted_label[0]\n",
    "\n",
    "# Example usage:\n",
    "image_path = \"/Users/zaravanthoff/Desktop/MasterProject/Datasets/full_dataset/full_images(2)/35_jpg.rf.73689f20496c4b1f1245e24e88b18a3e.jpg\"  # Replace with the path to your image\n",
    "predicted_logo = predict_logo(image_path, model, label_encoder)\n",
    "print(\"Predicted logo:\", predicted_logo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'load_img'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m# Example usage:\u001b[39;00m\n\u001b[1;32m     19\u001b[0m image_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/Users/zaravanthoff/Desktop/MasterProject/Datasets/full_dataset/full_images(2)/22_png.rf.722260fc209cbf978ada11b31cf7b58c.jpg\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# Replace with the path to your image\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m predicted_logo \u001b[39m=\u001b[39m predict_logo(image_path, model, label_encoder, target_size)\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPredicted logo:\u001b[39m\u001b[39m\"\u001b[39m, predicted_logo)\n",
      "Cell \u001b[0;32mIn[37], line 5\u001b[0m, in \u001b[0;36mpredict_logo\u001b[0;34m(image_path, model, label_encoder, target_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_logo\u001b[39m(image_path, model, label_encoder, target_size):\n\u001b[1;32m      4\u001b[0m     \u001b[39m# Load and preprocess the image\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     img \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mload_img(image_path, target_size\u001b[39m=\u001b[39mtarget_size)\n\u001b[1;32m      6\u001b[0m     img_array \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mimg_to_array(img)\n\u001b[1;32m      7\u001b[0m     img_array \u001b[39m=\u001b[39m img_array\u001b[39m.\u001b[39mreshape((\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39mtarget_size, \u001b[39m3\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'load_img'"
     ]
    }
   ],
   "source": [
    "target_size = (100, 100)\n",
    "\n",
    "def predict_logo(image_path, model, label_encoder, target_size):\n",
    "    # Load and preprocess the image\n",
    "    img = image.load_img(image_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = img_array.reshape((1, *target_size, 3))\n",
    "    img_array = img_array / 255.0  # Normalize pixel values\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(img_array)\n",
    "    \n",
    "    # Decode the prediction\n",
    "    predicted_label = label_encoder.inverse_transform([np.argmax(prediction)])\n",
    "    \n",
    "    return predicted_label[0]\n",
    "\n",
    "# Example usage:\n",
    "image_path = \"/Users/zaravanthoff/Desktop/MasterProject/Datasets/full_dataset/full_images(2)/22_png.rf.722260fc209cbf978ada11b31cf7b58c.jpg\"  # Replace with the path to your image\n",
    "predicted_logo = predict_logo(image_path, model, label_encoder, target_size)\n",
    "print(\"Predicted logo:\", predicted_logo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
